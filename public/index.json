
[{"content":"","date":"21 August 2025","externalUrl":null,"permalink":"/tags/controls/","section":"Tags","summary":"","title":"Controls","type":"tags"},{"content":"","date":"21 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"21 August 2025","externalUrl":null,"permalink":"/tags/ransomware/","section":"Tags","summary":"","title":"Ransomware","type":"tags"},{"content":" Premise # [DISCLAIMER: I\u0026rsquo;m the kind of writer that writes everything in one go. So please excuse any typos or else that might be present. You can always DM me on my various socials to report them and I\u0026rsquo;ll fix them ASAP]\nI think I\u0026rsquo;ve posted about these in the post on Twitter (now X), multiple times in fact, but I\u0026rsquo;ve always felt that this subject deserved it\u0026rsquo;s own little article. I\u0026rsquo;m going to try and keep it light, as I don\u0026rsquo;t intend for this article to be a full blown guide on how to address these controls, but just make you aware of them and what\u0026rsquo;s their state in SMBs in 2025.\nAs an active incident responder, I deal with ransomware every week. Have been doing this for the last few years now, mostly in the SMB realm and I can tell you this: AI-based threats (whatever that even means) is far from being the biggest worry when it comes to ransomware.\nWhich means, even any AI-based security solution won\u0026rsquo;t help you there when a threat actor gets in via valid credentials, and can just NetExec (or CME for the OGs?) his way through a domain controler before deploying the ransomware payload using PsExec.exe and a targets.txt.\nThe top missing and/or incomplete controls that could help prevent these attacks, or even cripple attackers hard enough to give defenders enough time to detect them (assuming you\u0026rsquo;re not missing #5 in the list) have, in my opinion, nothing to do with AI and/or whatever kind of security solutions some overhyped vendor tries to sell you.\nIt comes down to basic IT hygiene and best practices. And we\u0026rsquo;re not even talking about stuff like Microsoft Security Baseline or Active Directory Best Practices. Simply stuff that anyone can do without much overhead (yes, even you MSPs).\n#1 - No MFA on External Remote Service # The initial access vector I probably see the most. May it be VPN, RDP, RD Web or even Microsoft 365/Entra ID (when it comes to BECs), it is hard to imagine that MFA isn\u0026rsquo;t still implemented everywhere in 2025, yet here we are. You know what sentence I hear the most when I get called on an IR (ransomware or BEC) and the threat actor got in via simple username/password authentication?\n\u0026ldquo;We were just about to roll out MFA next week!\u0026rdquo;\nFor these organizations that did roll out MFA, I hear:\n\u0026ldquo;But how could this happen! We deployed MFA!\u0026rdquo;\nMore on that 2nd one in another blog post (hint: AiTM).\nIn 2025. Every single week/month, I hear that sentence. Every single day without MFA enabled on an external remote service is another day you may or may not be granted to pursue your business operations in peace. At some point, an attacker is going to get in that way. It\u0026rsquo;s just a matter of time. It\u0026rsquo;s not \u0026ldquo;luck\u0026rdquo;, it\u0026rsquo;s just that your number hasn\u0026rsquo;t been drawn yet.\nDepending on your setup, MFA isn\u0026rsquo;t that expensive to deploy. Even if you have to pay physical TOTP for some users. At the very least, it\u0026rsquo;s less expensive than dealing with a successful ransomware attack. The cost of which depends on whether or not you told your cyber insurance provider you had MFA deployed on your VPN \u0026hellip; when in fact, you didn\u0026rsquo;t.\n#2 - Exposed and/or Unpatched Edge Device # Probably the 2nd initial access vector I see the most (hello CVE-2024-55591!) after the lack of MFA. If there is one category of devices/appliances that should be patched as soon as updates/patches comes out, it\u0026rsquo;s the edge one. It is guaranteed that they\u0026rsquo;ll be targeted the moment a vulnerability and/or POC hits the street for them.\nSimply removing the exposition of the management interface (why is it exposed in the 1st place?) could help reduce the attack surface should new CVEs attack it and not another service that is available (e.g.: SSL-VPN).\nIn the SMB world, the management (read: including the patching) of these devices often falls on the MSP and/or IT partner which installed the device. However, unless the client decides to pay an extra in his monthly or yearly contract, usually, patching will be done on a \u0026ldquo;best effort\u0026rdquo; level \u0026hellip; if done AT all. That is, if patches are even available and the client isn\u0026rsquo;t running some EOL edge device.\n#3 - No Basic Network Segmentation # And by basic, I mean REALLY basic. Most organizations do have multiple VLANs or subnets, but they\u0026rsquo;re flat. Workstations can talk to servers on every port, and so can servers.\nRachel from Accounting can VPN in the network while working from home, and the user VPN subnet she lands in allows her to RDP (3389) to the domain controller directly.\nSo what do you think happen when a threat actor manage to get in through the VPN, using only a username/password (because of #1)? He can attack the servers right away, granted he wasn\u0026rsquo;t lucky enough to login to the VPN with an Active Directory account that already has some access to the servers.\nWe\u0026rsquo;re not even talking about DMZs here or else, but simple networking:\nHave your workstations in one subnet Have your servers in another subnet Your workstations should only be able to access specific services on the server subnet (e.g.: port 80/443 for the Web Server, port 1433 for the SQL Server, etc.). Your user VPN subnet should be similar to your workstations subnet, if not, even more restricted Your servers shouldn\u0026rsquo;t have unconstrained access to the Internet (hi to the person that made a tweet once where everytime he sees a Web Browser installed on a DC, he cringes) That basic network segmentation can go a long way. The less services/systems you expose, the smaller your attack surface is AND the bigger chances you have of weeding out threat actors in your network that starts lighting up your consoles like a Christmas tree. Assuming you have someone looking at that Christmas tree. More on that in #5.\n#4 - Improper Configuration of \u0026ldquo;Service\u0026rdquo; Accounts # I always use air quotes when talking about \u0026ldquo;service\u0026rdquo; accounts, because at the end of the day, we all know what they are. Not Managed Service Accounts (MSAs), not gMSA, not sMSA. They are simple Active Directory users (or local users) with the \u0026ldquo;svc\u0026rdquo; string in their name. From there, their credentials were plugged in whatever system or service they need to power and we call it a day.\nThese accounts often have high privileges (why is svcsql a Domain Admin?) and they aren\u0026rsquo;t restricted/limited in what they can do on the network (why can svcExchange RDP to the DC? Why can it RDP AT ALL?). So of course, these accounts are highly interesting for threat actors, because they know that if they compromise only one, it\u0026rsquo;s practically already game over at this point. Somehow that account can be leveraged to compromise an actual DA account, when it can\u0026rsquo;t just straight up RDP to a DC, and then the Kingdom will have fallen.\nIn an ideal world, you should be using your flavor of MSAs and not actual, simple users. However, if you are, do take the proper steps to secure them. Make it so they can only log in to specific systems, they can\u0026rsquo;t RDP at all, they have long and complex passwords, etc. And if someone ever tries to use them in an interactive fashion (e.g.: RDP), alert on it. But that may be too much to ask, SMBs don\u0026rsquo;t really have a SIEM after all. Subject for another blog post.\n#5 - No AV, NGAV, EDR \u0026hellip; or Even Someone to Look at The Alerts # I have never seen a SINGLE incident in my professional life so far where NO alerts where generated during an incident. Assuming an AV, NGAV or EDR was deployed that is. Even on Windows versions where Windows Defender is built-in now, it\u0026rsquo;ll always have raised alerts. Granted if the organization is running Defender \u0026ldquo;standalone\u0026rdquo; (no SCCM, no Intune, no MDE P1, etc.), well, unless the user actually notices the Defender pop-up when there\u0026rsquo;s a detection \u0026hellip; But there will always be a detection. And keep in mind that even when a threat actor disables or uninstall the 3rd party endpoint security solution on a system, Defender will be turned back on. Which means, there are very good chances it\u0026rsquo;ll start triggering on what the threat actor is doing.\nMost SMBs don\u0026rsquo;t have an EDR and still run whatever AV/NGAV their MSP or IT provider offers them (hello Webroot, Bitdefender, ESET and all that!). However, it seems like when these products are sold to the client, they\u0026rsquo;re sold to check a bullet point in a list and that\u0026rsquo;s it. No monitoring and/or review of the alerts is done and if there is, they are mishandled anyway (read: the person looking at the alert doesn\u0026rsquo;t understand what he\u0026rsquo;s looking at).\nJust having a decent security company do triage on your endpoint protection solution alerts can do a long way. At the very least, they\u0026rsquo;ll know that it\u0026rsquo;s odd for SMB\\reception1 to execute mimikatz.exe out of C:\\Users\\Public\\Music. Well, I hope so.\nConclusion # So if you are a SMB reading this, or even a MSP/IT provider, the best thing you can do right now to help protect your clients against ransomware attacks, or even make it harder for threat actors to execute their goals once they\u0026rsquo;re inside, is to work on these 5 points above. Depending on the environment, some of them, if not all, may be easy to implement, while others may be a bit harder. At the very least, when it comes to #3, should you deploy a network environment with a \u0026ldquo;basic security\u0026rdquo; first mindset from the get-go, it makes things way easier down the road.\nYou are free to do whatever you want with that information and my opinion. At the end of the day, I know I\u0026rsquo;ll continue to deal with incidents where at least one, if not multiple or all of these controls are absent and allowed an attack to go through. So I may as well put it out there for anyone who wants to know what its like to deal with ransomware in SMBs in 2025. So you can see for yourself that there\u0026rsquo;s nothing really \u0026ldquo;extraordinary\u0026rdquo; about it.\n","date":"21 August 2025","externalUrl":null,"permalink":"/posts/1755787102398-ransomware-in-smbs-top-5-missing-or-incomplete-controls-that-could-help-prevent-or-cripple-attackers/","section":"Posts","summary":"","title":"Ransomware in SMBs: Top 5 Missing or Incomplete Controls That Could Help Prevent or Cripple Attackers","type":"posts"},{"content":"","date":"21 August 2025","externalUrl":null,"permalink":"/","section":"SecurityAura Blog","summary":"","title":"SecurityAura Blog","type":"page"},{"content":"","date":"21 August 2025","externalUrl":null,"permalink":"/tags/smb/","section":"Tags","summary":"","title":"Smb","type":"tags"},{"content":"","date":"21 August 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"Hey! My name is SecurityAura and I\u0026rsquo;ve been working in Information Security for almost 10 years now. I\u0026rsquo;m a DFIR Consultant and therefore, active incident responder. I mainly deal with intrusion, data exfiltration, ransomware and Business Email Compromise (BEC) cases. I also assist our Microsoft Sentinel and SOC teams with advanced investigations and use cases development. The rest of my time is spent on Threat Hunting and Detection Engineering.\nKnowledge, Skills \u0026amp; Interest # As far as my knowledge and skills goes, I focus on:\nMicrosoft Defender XDR Microsoft Sentinel EDRs (Carbon Black Cloud, CrowdStrike, SentinelOne, MDE, etc.) Digital Forensics Incident Response Threat Hunting Detection Engineering Microsoft PowerShell (which I use for all my scripts) Which are therefore all domains I\u0026rsquo;m interested in.\nSocials # I\u0026rsquo;m mostly active on Twitter, but I also have a BlueSky and Mastodon account on InfoSec.Exchange:\nTwitter (X): https://x.com/SecurityAura Bluesky: https://bsky.app/profile/securityaura.bsky.social Mastodon (InfoSec.Exchange): https://infosec.exchange/@SecurityAura And I have a few repos, namely one dedicated to DE\u0026amp;TH (Detection Engineering and Threat Hunting) using the Microsoft Defender XDR and/or Microsoft Sentinel stack (read: KQL stuff), on Github:\nhttps://github.com/SecurityAura\n","date":"11 August 2025","externalUrl":null,"permalink":"/about-me/","section":"SecurityAura Blog","summary":"","title":"About Me","type":"page"},{"content":"","date":"11 August 2025","externalUrl":null,"permalink":"/tags/example/","section":"Tags","summary":"","title":"Example","type":"tags"},{"content":"","date":"11 August 2025","externalUrl":null,"permalink":"/tags/tag/","section":"Tags","summary":"","title":"Tag","type":"tags"},{"content":"Living your life, 1 query a day, for 100 days\nPremise # This blog post is about my #100DaysOfKQL challenge that I did from January 1, 2025, to April 12, 2025. For this challenge, every day I had to post a KQL query in my Github repo:\nGitHub - SecurityAura/DE-TH-Aura: Repository where I hold random detection and threat hunting…\n_Repository where I hold random detection and threat hunting queries that I come up with based on different sources of…_github.com\nThat challenge was inspired by Matt Zorich (@reprise99 on Twitter/X) who did it a few years ago, but he actually went for 365 days. The #100DaysOfX challenge is quite popular, having people doing it this year for YARA as well.\nIn this blog post, I’ll be looking back on that challenge, how I lived it, what I learned and also, what future awaits my Github repo.\nAlright, boring stuff out of the way, moving on!\nOn a 1 to 10 scale, this was- # Holy was that kind of challenge hard. I have no idea how Matt (@reprise99 on Twitter/X) did it for 365 days straight, the absolute madman, but even just 100 days was enough to drain me of a lot of energy. And even there, it isn’t a real 100 days since I actually missed 2 days. For the first one, I had been up for 36 hours straight since my youngest was sick, and in the evening, I had a choice: put out a query, or go to bed. And I don’t regret my decision obviously, sleep is important. The second time could’ve been avoided to be honest. A new incident response mandate came in that day, I spent the whole evening on it and as I logged off to go to bed, noticed there was only 30 minutes left the day so … yeah. This is what happens when new IR mandates come in. The craze, the adrenaline, the excitation for the unknown. You forget about everything else.\nThe first 50–60 days or so I think went well. I’m generally happy with the queries I put out and what they can do. One thing you need to know about these queries is that either I had them already written up from previous experience, or I was writing them on the fly on the same day I was releasing them. And I just don’t write a query, post it and #YOLO hope it works. No, I test/reproduce the behavior I want to catch in my home lab to make SURE that the query works as intended. This is why putting these out wasn’t an adventure of a few minutes per day. It could take me up to 1 or 2 hours everyday to craft these because I had to do the testing and fine-tuning.\nAt some point though, you’ll notice that the quality of the queries, or at least, the page they came in, started falling short. This is where it started to get hard for me for various reasons, most of which were health-related (sickness, fatigue, etc.) most of which I’ll talk about in a bit. However, I do have plans for these, because I would not be OK with myself to leave to leave them in their current form.\nThe obvious time constraint (x2) # For those of you who don’t know, I’m a dad of two young amazing kids, which means, I do not have as much free time as I used to have back then. At some point, when they grow older, I’ll have more free time. But right now, free time is basically 2–3h in the evenings during the week, 2–3h in the afternoon on the weekends (afternoon nap) and your usual 2–3h in the evenings. And at some point, the afternoon naps are going to stop which means I’ll only have my evenings. Another thing about me is that I’m a workaholic. Which means, most of my evenings I’ll spend working because, there’s just so much to do, and that I want to do. And I’m way more productive in the evenings than the day (“golden hours” per my girlfriend).\nAn impression of déjà-vu # Those who have been paying attention to the queries may have noticed that sometimes, there will be a few queries centered around the same concept. May it be Windows Services, Scheduled Task, low prevalence binaries, etc. They may look similar to one another, but the reality is that, depending on where (which environment) you use them and what you are looking for, they may give entirely different results or even serve a different purpose. Let’s take Windows Services as an example. In total, I put out 4 queries for it:\nDay 22 — Windows Service Creation or Modification With binpath via sc.exe.md\nDay 33 — Suspicious String in Service Creation ImagePath.md\nDay 44 — Windows Service Created Remotely.md\nDay 87 — Command Line Interpreter Launched as Service.md\nFYI: I just noticed the 22, 33, 44 suite while writing this blog post. It was not intended, but it made me chuckle.\nDepending on your environment and how Windows Services are leveraged, the 4 of them could be used as detections or Threat Hunting queries. They are just 4 different ways to detect and/or hunt for suspicious/malicious behavior for Windows Services. You could literally, let’s say, come up with a Threat Hunting Playbook around Windows Services and each of those would be a valid query to run. And it doesn’t stop there. You could come up with queries such as:\nWindows Service Created with a BinPath in an Usual Folder\nWindows Service Spawning a Low Prevalence Process\nWindows Service Spawning a Non-Signed or Invalidly Signed Process\nEtc.\nAll of these possible variants is what allows you to come up with an insane amount of queries that can be used as either detections or hunts within your environment. You just have to find out what works for your organization and put it in place.\nBuilding Blocks # A lot of the queries I put out are unfinished. By that I mean that I consider them to be base queries that can be built/improved upon. Why? Because depending on your organization, the base query may work as it is, or it may need further fine-tuning/refinement.\nLet’s say the use of certain LOLBIN is strictly controlled in your environment through various means, having a query looking for these could be good enough to be turned into a detection right away, because you know that these should never, or very rarely, be used. On the other hand, if LOLBIN are used left and right in your environment that you cannot just hunt for their standard use, you’ll have to come up with a query (or multiple queries) that targets the very specific suspicious or malicious behavior you want to detect.\nA base query is always good to get you started on a hunt, but if you want to turn it into a detection (if possible), it may need to be a developed a little bit more.\nI even ended up rewriting some queries I had already written back then, because I had found ways to make them better in some fashion. Hence why you can see that some logic also comes back in multiple query. As with everything I create, I don’t consider the queries I put out to be in their final form. Just a form that worked for me at the time I put them out, and that I can always come back to in order to improve them at a different time.\nWhat’s next then? # You may be wondering what’s next with the GitHub repo following the end of the #100DaysOfKQL challenge. Worry not, I have a lot of plans for it that are just constrained by the free time I have available, as explained previously. Right now, there are three (4) things I want to do with the DE-TH-Aura repo:\n1. Go back to queries that were posted hastily and lack substance and complete them. May it be in terms of description, example, references, etc.\n2. Review each queries to make sure that they have the appropriate level of information, in term of content. For instance, add the TTP table to each one of them like @BertJanCyber and many others do already.\n3. Add additional queries for those that can be done across multiple products. For instance, if I have a query where I use DeviceLogonEvents, obviously it can also be done through the Windows Security Event Logs using SecurityEvent.\n4. Create a repo architecture (folders) that will allow me to move/class queries where they belong.\nRegarding #4, I’m basically talking about something alongside of let’s say, moving queries that uses DeviceProcessEvents to a Defender for Endpoint (MDE) folder. However, is that the only thing I want to do? Do I also want to further classify them into let’s say MITRE ATT\u0026amp;CK TTPs? However, how can I do that when a query hits multiple TTPs? Even more so: what do I do with query pages that have queries for Defender XDR products, but also SecurityEvent? SecurityEvent isn’t part of the Defender XDR suite, so should they still be attached to the page and be moved in a MDE folder? As you can see, I haven’t exactly worked out how I want to organize the repo and it’s something I can get stuck on for a long time. That’s just the kind of person I am. However, before I even get to #4, I still have to go through the first steps which means, I don’t have to make a decision right now.\nObligatory shoutouts # I think it’s customary in these kind of posts to give some kind of shoutouts as well. At the very least, I want to highlight a few people here:\nMatt (@reprise99 on Twitter/X) for inspiring me to do this challenge with his (you’re crazy, never doing it again)\nBert (@bertjancyber on Twitter/X) for having such an amazing repo and crafting a template that anyone can use for theirs\nJamie (@jamieanticosial on Twitter/X) , Andrew (@4ndr3w6s on Twitter/X) and Anton (@Antonlovesdnb) for being my #1 fans who ALWAYS liked my tweets first. At some point I suspected that Jamie was using some kind of macro or else to like my tweets not even 5 seconds after I pressed that Post button.\nNathan (@NathanMcNulty on Twitter/X ) and Dylan (Dylan) for replying to my tweets with the right Github URLs because I had changed the page name (and therefore, URL) after posting about it. Silly me, teehee~~~ (or say they say now I think).\nAnd a lot more people in the background who reached out to me to say how they appreciate the challenge. Shoutout to these folks in Curated Intelligence (@CuratedIntel on Twitter/X) as well.\nThe End! # If there’s anything I learned from this challenge is that there are just so many ways to write queries in KQL and there are so many things you can do with it if you just use the right filters, functions, etc. As someone who does not come from a programming background (I write PowerShell and I like it, that’s as much as I can do) I felt like KQL was easy enough to pick up and learn, on top of being flexible and powerful enough that it allows me to do what I need to do with it. I even ended up rewriting old queries I had created because while writing a new one, I came up with a better way of doing something and just wanted to apply that logic in every query where it was used.\nOh and obviously, never doing that kind of challenge again. At least, not until the kids moves out of the house which will be … in a while.\nTill next time!\nPS: Ann Fam feel free to DM me any typos you find in this post, because considering how fast and unbothered I wrote this (there’s no DEV, only PROD), there’s bound to be a LOT!\n","date":"17 May 2025","externalUrl":null,"permalink":"/posts/@securityaura/looking-back-on-100daysofkql-bf526b3d214e/","section":"Posts","summary":"","title":"Looking Back On #100DaysOfKQL","type":"posts"},{"content":" Disclaimer # Obligatory disclaimer that English is not my first language and I wrote this blog post in one go with no QA (re-reading) afterwards. So if you spot any kind of mistakes, typos, etc. (HI Ann Fam!) feel free to DM me on Twitter (or X as it’s called now).\nAlso, the kind of BECs I’m talking about here are your run-of-the-mill BECs that ended up in financial fraud of some kind, or the threat actor leveraging his access to send phishing emails (internally and externally). Not really dealing with APTs here.\nPremise # The idea, or content if I can say, for this blog post mostly comes from a tweet EricaZelic posted earlier today.\nSince I had just spent the whole day going through two (2) huge UAL for two (2) Business Email Compromise (BEC) I had been working on, and took some time to update my (numerous) parsing and analysis scripts, the ̶r̶a̶g̶e̶ insights were still fresh in my mind.\nTherefore, this blog post will highlight a few things you should know about UAL and in some situations, how you can deal with them.\nSo Many IP Addresses (But Not Always) # If you’re like me and like to tell clients that in the UAL, every event is linked to an IP address so that’s how you can identify events generated by an authorized third-party, well, welcome to the liar’s circle.\nThe truth is that not every event in the UAL has an IP address associated with it. And you can see this in two (2) ways: some events simply do not have a field holding IP address information, while some do have a field for ClientIP (or else) but it’s simply empty (you can’t imagine how hard I’m holding back writing this line).\nAnd for these events that have an IP address, you can actually have multiple JSON fields representing that IP. The most common ones are: ClientIP, ClientIPAddress and ActorIPAddress. So depending on the Operation type, the field holding the IP address information may change. More information about this can be found in the Microsoft Learn page for the Office 365 Management Activity API Schema:\nOffice 365 Management Activity API schema\n_The Office 365 Management Activity API schema is provided as a data service in two layers - Common schema and…_learn.microsoft.com\nNot All IP Addresses Are Equal # From there, if you’re lucky enough to have a UAL with load of IP addresses, you’ll face your 2nd hurdle: their variety. Within a full UAL for a user (may it be for 30, 60, 90 days, etc.) you can get the following kind of IPs for all sort of Operations:\nLocal (RFC1918) IPv4 Localhost/Loopback IPv6 (::1) Link-Local IPv6 (fe80::) IPv4 (e.g.: 20.30.40\\[.\\]50) IPv6 (you all know what an IPv6 looks like) IPv4 with port (e.g.: 20.30.40\\[.\\]50:12345) IPv4 with the last octet missing (e.g: 20.30.40.x) So right off the bat, remember that statement about being able to identify events from an unauthorized party by looking for events with “sus” IP addresses? Good luck with that.\nBut anyway, you have IP addresses, great! They are just random numbers that means absolutely nothing until you enrich them obviously. Only from there would you be able to determine if they’re truly “sus” or not (and even there, more about this later in this post).\nSo fire up your favorite IP enrichment API and pass it that list of unique IP addresses to get the information you need. Spur.us is a GREAT product when it comes to identifying IP addresses associated with anonymization solutions and/or products (read: VPN). Or you can also flag them based on ASNs that are known to have a bad reputation, are abused by threat actors and/or are just outright known to be associated with VPN providers. Something like IPinfo.io with their free tier can definitely assist with that.\nIt’s Always Microsoft # Another issue you’re going to face when identifying these IP addresses is trying to identify the ones that are related to Microsoft-related services, may it be Office or Azure. Luckily for us, Microsoft has two (2) files you can use to help see if an IP is part of a Office and/or Azure IP range.\nThe Microsoft Public IP Space:\nhttps://www.microsoft.com/en-us/download/details.aspx?id=53602\nThe Microsoft Azure IP Ranges and Service Tags — Public Cloud:\nhttps://www.microsoft.com/en-us/download/details.aspx?id=56519\nThe first one is a CSV file and the second one a JSON because … Because? Yeah, why? At this point I don’t care, all you need to know is that whatever automation you’re planning on building to handle the lookup, you’ll have to handle both CSV and JSON files.\nNow that you identified which IPs are associated with Microsoft and marked the UAL events associated with them as such, you think you’re good to go, right? Do you really think I would go through the trouble of writing a blog post at 10 PM on a Monday night if it was the case? Of course not. After doing this exercise, you’ll realize that Microsoft has a lot, and by that I mean, A LOT, of IP addresses that are undocumented (as I call them). By that I mean they aren’t IP addresses that fit in any of the prefixes from the two (2) previous files. So you’re basically back to IP address enrichment (OrgName, ASN, etc.) to try and filter out these remaining offenders. Because nothing is ever easy with Microsoft.\nThe Inconsistency is Real # Now that you’ve identified all your IP addresses and flagged suspicious and/or malicious events, you think that all is well that ends well, right? Well, not quite.\nNow you basically need to dig into these UAL events one-by-one (literally, or not, depending on which road you go down) to try and identify what exactly happened and what are the implications.\nTo do so, you need to dig into the AuditData field (if you exported the UAL as a CSV) which is just a big JSON blob with all the information related to an event. You’ll start dissecting some events and notice interesting fields such as ApplicationId, SessionId, ClientAppId…? AppAccessContentAPIId? AppAccessContentClientAppId? Why do some of these fields feel like they’re the same, but with a different name?\nBecause this is exactly what this is. At least, based on my research into two (2) UAL files today which I finally took the time to do. Some of these fields hold the exact same kind of information/value but depending on the Operation type, the fields that are present, used and populated will differ. From what I can see at the very least:\nApplicationId, ClientAppId and AppAccessContextClientAppId will either have one of these field populated, but if more than one is, it’ll be the same value AppAccessContextAADSessionId and SessionId don’t seem to have an overlap, it’s either one or the other. But the values you get basically covers the “same” sessions. So for instance, you can have events where the AppAccessContextAADSessionId is set and others where that value is in the SessionId field At some point (read: when I stop being lazy), I’ll post a small comparation table into these different Operation types and fields on my Github so you can see for yourself. The result isn’t pretty obviously.\nCorrelation Goes A Long Way # Now, remember that statement where if you filter out events from Microsoft IPs, you’re “good-to-go”? Well, I’m going to confuse you even more.\nWhat I often see in my BECs is the threat actor logging in to the account through either OfficeHome (which is portal\n\\[.\\]office\n\\[.\\]com) or Office 365 Exchange Online (which should be outlook\n\\[.\\]office\n\\[.\\]com). In instances where the AppId points to OfficeHome, you may also get FilePreviewed and FileAccessed events. Which, from what I remember in my previous testing, would be because of these “last modified”, “last accessed”, etc. files that are displayed in your Office portal when you log in. You’ll notice that these events are actually associated with Microsoft-related IPs. However, if you look at the CreationDate for those, and the User-Agent, you can actually associate them with UserLoggedIn events for the OfficeHome AppId.\nSo a good strategy in identifying those, because I should at least give some tips and tricks here and not just baseless hate, is having your filters set for the AzureActiveDirectoryStsLogon record types (UserLoggedIn is part of it) and the SharePointFileOperation (which FilePreviewed and FileAccessed are part of). From there, order the events in a chronological order and if you see any login events with the same CreationDate as FilePreviewed and FileAccessed events, confirm that the User-Agent field is the same for these 2–3 Operation type of events. And if they are, you can cluster these events in the batch generated by the threat actor.\nAnd some obviously bad events, such as the threat actor “installing” (read: consenting) to an application in Azure (e.g.: eM Client, PERFECTDATA, etc.), actually comes from Microsoft-related IP addresses, when they have one. One of the UAL I looked at today just don’t have any IP address associated with these. So if your analysis is based on event identification through IP address enrichment, you’ll end up missing those.\nAh, and don’t forget these events from IPv4 addresses with port (20.30.40\n\\[.\\]50:12345) … you don’t want to miss those either.\nOkay, UAL is Crap. Now What? # Despite everything, and because of everything, the UAL still remains honestly the best source of information you can have when investigating BECs. Assuming:\nThey are enabled (look at this post from NathanMcNulty on his website: https://blog.nathanmcnulty.com/azure-automation-advanced-auditing/) The proper auditing configuration is in place (refer to Nathan’s blog post above as well and/or his tweets) You’re ready to spend a̶ ̶h̶u̶g̶e̶ ̶a̶m̶o̶u̶n̶t̶ ̶o̶f̶ ̶t̶i̶m̶e̶ small amount of time parsing it, enriching it and reviewing it For everything else, or if you don’t have the time nor resources to do the above, the best combo you can use right now is the Microsoft-Extractor-Suite from Invictus Incident Response.\nGitHub - invictus-ir/Microsoft-Extractor-Suite: A PowerShell module for acquisition of data from…\n_A PowerShell module for acquisition of data from Microsoft 365 and Azure for Incident Response and Cyber Security…_github.com\nAnd the Microsoft-Analyzer-Suite (which uses MES UAL output) from evil3ad79 (https://x.com/Evild3ad79):\nGitHub - evild3ad/Microsoft-Analyzer-Suite: A collection of PowerShell scripts for analyzing data…\n_A collection of PowerShell scripts for analyzing data from Microsoft 365 and Microsoft Entra ID …_github.com\nIf you are a SMB, an organization with little to no resources for dealing with BEC or even an Analyst with not a lot of time to pour into investigating this, the MES and MAS combo can basically save you a whole lot of time.\nObviously, if your investigations requires more analysis in order to fully identify and understand what happen, you’ll still have to dig into the UAL, it can’t be avoided.\n/Rant # And that’s the end of my rant for tonight. Special thanks to DylanInfoSec (https://x.com/DylanInfosec) who actually pushed me to write this blog post. Give him a follow, alongside everyone else I tagged and/or mentioned in this post. They’re all awesome human beings, amazing at what they do and I promise you’ll learn so much from their posts it’s crazy.\nhttps://x.com/DylanInfosec/status/1868848348876448043\nJoking aside for a moment, I wouldn’t be able to investigate BECs properly without the UAL and I learn something new about them every single time. So if you have the time and curiosity, I highly suggest you start digging into the UAL and do research of your own. The more UALs you go through, the better you’ll get at reviewing them and the faster you’ll become as well. As with pretty much everything in life I guess. Grunt work is best work!\nPS: I may be doing more blog posts about the UAL in the future so … stay tuned?\n","date":"17 December 2024","externalUrl":null,"permalink":"/posts/@securityaura/microsoft-unified-audit-log-ual-what-you-come-to-learn-the-hard-way-905a087a1558/","section":"Posts","summary":"","title":"Microsoft Unified Audit Log (UAL): What You Come To Learn The Hard Way","type":"posts"},{"content":" Premise # Warning: Wall of text ahead! Sorry for the screenshots/images lovers (and I am one!).\nThe information presented in this article, on top of the various insights offered comes from my few years of experience in Incident Response. It is not an absolute truth and there are many different ways to answer the questions presented here. However, this article aims to give insight in what kind of questions are asked in incidents where data exfiltration occured or may have occurred and ways to answer them.\nThis article is mainly based on ransomware and/or data exfiltration incidents at a host and/or network-based level.\nAt some point in time, I may post other articles related to data exfil with additional insights and/or stories.\nIntroduction # Whenever you deal with an incident where a threat actor managed to get access to an environment, one of the biggest investigation track you’ll have to work on is the data exfil one. In ransomware incidents and/or incidents where a victim received a ransom demand, the first and biggest question that will be asked is: did data exfiltration occur? In these incidents, if you can quickly identify which threat actor you’re dealing with, you can already assess with a good level of confidence if data was exfiltrated or not. That’s why, at the beginning of these incidents, one of the first steps will be to call in a breach coach. From an incident responder’s perspective, the breach coach is seen as the party that will assist the victim with assessing the risk of a data exfil and the data that would’ve been exfiltrated. They’ll also provide guidance on the notification process, should people whose data have been “stolen” need to be contacted.\nWhile a lot of companies (even SMBs) may think that they do not hold any kind of sensitive data, such as PII, PHI, etc. and therefore, notification to impacted people would not be necessary, I typically ask them if they have any kind of HR and/or payroll information. Most of them will say yes, meaning that in these situations, these people (employees, ex-employees, contractors, etc.) would need to be notified. It shows that even in 2024, companies may not even be aware of the kind of data they hold, nor what they represent and their classification.\nThe Questions # When it comes to data exfil, multiple questions need to be answered or in short: anything and everything. For the purpose of this article, you can see these questions as:\nIf — Did data exfil even occur? This is usually confirmed (or infirmed) by answering any of the remaining questions How much — How much data was exfiltrated? Are we talking about a couple of documents? A few MB? Or multiple GB, even a whole TB? How — How was the data exfiltrated? Was it uploaded to a cloud service? Was it sent via SFTP to a remote server in another country? When — At which point in the incident did the data exfil occur? How long did it last? In ransomware cases, was it just before the ransomware payload was deployed? Was it days before? What — The hardest question of all the answer: what data was exfiltrated? Which files, folders, network shares, etc. were exfiltrated by the threat actor during that attack? The ability to answer these questions depends on so many factors (even a bit of luck sometimes) that it is impossible in my opinion and experience to go in a new incident and tell a victim that you’ll be able to answer all of them. You’ll always do your best at answering as many of them as you can in the most complete way possible too.\nIf — Did data exfil even occur? # Depending on the incident and what kind of information is available to you when you get called in, you may already have the answer to that question. Or at very least, can heavily suspect the answer.\nFor instance, if you get called in on a ransomware where you can quickly identify the threat actor group (through the ransom note, encrypted file extension, etc.) and you know that this group is known to leak/publish stolen data (e.g.: a Dedicated/Data Leak Site (DLS)), you already know that chances of data exfil are high.\nOtherwise, answering this question in a positive or negative fashion is usually dependent on the other questions. If you are able to positively answer any of the other questions, you pretty much confirmed that data exfil occurred.\nHow much — How much data was exfiltrated? # Alongside the “What”, one of the hardest question to answer. Answering this question is typically highly dependent on having a way to measure/calculate the amount of data that went out of a network. May it be through firewalls, NDRs, the ISP and/or even host-based artifacts (such as SRU).\nMost next-gen firewalls now have very nice dashboards that can help you quickly see how many GB of data was downloaded/uploaded in various situations: top source, top destination, top application, top web filter, etc. This is probably one of the easiest way to answer this question. For instance, if you see that the Top Source in the firewall dashboard is the file server that uploaded 100 GB of data, and when you drill down on the details, you see that 90 GB of that data was uploaded through port 22 to an IP address in another country that has no association with the victim, you can already deduce what you should do next. Investigate that server for artifacts which would show that data was explicitly sent out by the threat actor (or at least, through means or actions not known to the victim).\nFor firewalls and/or NDRs that do not have dashboards and/or reports available, you can usually still answer that question using the raw logs. If you can export the capture traffic logs, and these logs have the sent/received bytes fields, and you know how to interpret them (e.g.: are the values incremental), you can whip up a quick script in your favorite language and/or throw the logs in your favorite SIEM to calculate the amount of bytes that was sent out. Therefore, giving you the possible amount of data that was exfiltrated.\nAnd in situations where the victim (or you, the Incident Responder, depending on how you look at it) does not have firewall logs available (e.g.: logs were in memory and it was rebooted before you get called in) or a NDR, from a network perspective, your last hope would be to request the logs from the ISP. If you’re lucky, the victim’s ISP will be able to generate bandwidth usage and/or download/upload reports for the last X days (covering the attack period). From there, if you’re able to answer the “When”, you may be able to get a glimpse as to how much data could have been exfiltrated, at a maximum. For instance, if you find out that data exfil occurred on the 14th of the month, and the ISP report shows that on that day, 150 GB of data went out, you can say that at the very least, a maximum of 150 GB of data (of an unknown type: raw files? archives?) could have been exfiltrated.\nLast but not least, host forensics could provide you with that answer if you’re lucky enough to access certain artifacts which would help you quantify the amount of data that was sent out (e.g.: SRUM). However, when the data exfil is done on a Windows Server, SCRUM is rarely enabled and therefore, data collected.\nA few other ways to answer that question can also be found in answering the “What” one.\nHow — How was the data exfiltrated? # The “How” question usually gets answered relatively quickly and/or easily as you investigate the incident. For instance, when performing host forensics on a system and looking at the UserAssist artifacts for a user you know the threat actor used, and find that he executed WinSCP.exe or rclone.exe, you already have the beginning of an answer.\nFrom a host-based perspective, reviewing the processes that were executed during the timeframe of an attack will most often yield the answer to that question. If the exfil was done through a program and/or process that is typically associated with data exfil (such as WinSCP, rclone above), chances are it’ll show up in your forensics.\nFrom a network point of view, you may already be able to answer that question or at least, have an idea on how to answer it if you were able to answer the “How much” one. If you find out that the file server sent 90 GB of data to a remote server outside of the country via SSH, you can then use that information in your host-based analysis to focus on ways a threat actor could exfil data through SSH on a host (ex: WinSCP, rclone, even pure SCP).\nOverall, that question is usually one of the easiest to answer, assuming anti-forensics wasn’t performed or the systems on which data exfil occurred were not encrypted. Ransomware can encrypt a lot of artifacts and evidences that would be useful in determining how data was exfiltrated, which can make it easier to answer that question.\nWhen — At which point in the incident did the data exfil occur? # Another question that can usually be answered relatively easily through the “How much” and “How” questions.\nThe easiest way to answer it being through network analysis. If you have access to firewall logs and can identify exactly when the first SSH connections to that remote server started and when they ended, you have yourself the timeframe on which the data exfil took place. You can then use that information with other sources of information such as the ISP bandwidth usage and/or download/upload reports. If these reports are broken down by hours for instance instead of days, you may be able to get a more accurate number when it comes to the size of the data that was sent out. In situations where the data exfil started on one day, for instance, on the 15th, and ended on another, let’s say the 16th, if the ISP reports are broken down in 24h periods, you would have to addition together the full size of both days of upload to get an estimate as to how much data, at maximum, could have been exfiltrated.\nFrom a host perspective, determining when the threat actor was active on the compromised system from which the exfil was done from will yield that answer. Depending on the artifacts available, you may be easily able to determine when the data exfil started. For instance, if rclone was launched on the 14th at 4:00 PM UTC, you know that at the very least, this is when the data exfil started. From there, you would need to find another indicator which you could use to determine the end of the exfil. For instance, when the SRU stopped logging information for the rclone process and/or when the ransomware payload was launched. Usually, the data exfil will be finished before the threat actor launches the ransomware. Otherwise, the data they’re trying to exfil would just end up being encrypted, which kinds of defeat the purpose.\nWhat — The hardest question of all the answer: what data was exfiltrated? # The whole “data exfil” concept aside, by far, the most difficult question to answer. And sadly, the most important one for the victim, but also the breach coach. Your answer to this question, or whatever information you can get to try to answer that question will often determine the next course of action for the victim and the breach coach when it comes to risk assessment (for the exposure) and the notification process.\nI’ve never been able to answer that question at 100% on all the incidents I’ve worked on so far. And I doubt that anyone else did (but if you did, please share with me how you do it because that is one impressive feat!). If I was to breakdown what allowed me to answer this question in its entirety, or partially, in situations it would be:\nNo anti-forensics from the threat actor (read: not even cleaning up after themselves) Good ol’ forensics (network or host-based) Unexpected help The first situation is definitely one of the easiest way to answer that question. In incidents where the threat actor staged the data before exfiltrating it (e.g.: creating ZIP, RAR, 7Z, etc. archives), if the archives are still present when you get called in, all you need to do is grab them and pass them to the victim for its review. This also allows you to answer the “How much” question at the same time, since you have the size of the archive(s), but most importantly, their content.\nAs far as forensics goes, depending on how the data exfil (and even data staging!) was done, there may be ways to replicate and/or even emulate it with the evidence you’ll uncover. For instance, if the threat actor used a PowerShell command and/or a script to stage the data or exfil it by targeting the exfil process/binary at a folder or server, you can reproduce these commands or run the script (modify it for a “report-only” mode, kind of) in order to get an idea of what data would’ve targeted. This method has caveat obviously in the sense that, depending on the commands that were executed, the time at which they were executed, etc. your results may not yield at 100% the data the threat actor obtained when he executed them at his point in time. For instance, if a rclone command that grabs file that were modified in the last 3 months to exfil them was ran, but you find this it 1–2 weeks after the the fact, you would need to adjust it to target files as accurately as possible. Same thing if you manage to uncover (or recover) a script that was used to staged data by invoking WinRAR to create archives. If you have the WinRAR commands and the folders it targeted, you can replicate it, create the archives and from there, have a good idea of which files would’ve been included in the threat actors’ archives when they were exfiltrated.\nLast but not least is … luck (or unexpected help, depending on how you want to see it)! Depending on the environment you investigate, you never know which solutions, configuration and/or logging are in place. And amongst this, there may be something that could assist you in answering the dreaded “What” question. I worked on an incident where an Application Whitelisting solution was deployed on the server where the data exfil was executed from, in learning mode. So while it did not block anything, it did log the 50,000+ files that were accessed by the WinSCP process that was used to send the data to a remote server over SFTP. The lesson to retain here is to make use of anything and everything you have access to. This is why a good understanding of the environment you’re investigating is critical since you may be able to identify something that could help you answering this question.\nConclusion # As I finish writing this article, I realize that it may be a tad bit longer than what I intended it to be. At the same time however, there is so much more that could be said on the subject and the various questions that each of them could deserve its own article.\nIn the end, while the data exfiltration angle can be hard to investigate, there is multiple ways to go at it. In a perfect environment where you have a firewall, an EDR, a NDR, etc. answering most of these questions should be a piece of cake. However, the reality is (at least, mine) that this kind of setup is an utopia for the most part, even more so when it comes to SMBs. At most, you’ll have access to decent host-forensics and firewall logs, but that’s it. That’s why knowing how to answer these questions with what you have is important, but even more so, being able to verbalize and explain to the victim and breach coach the limitations and restrictions when it comes to investigating data exfil. That often, you may not get answer to all these questions, nor even one of them.\nThis being said, hopefully this article exposed you to my bit (and the bit of many other incident responders!) of reality when it comes to dealing with incidents involving data exfil and even given you ideas on how to investigate your own (right now, or the next one(s)!).\nTill then, see you next time!\n","date":"15 September 2024","externalUrl":null,"permalink":"/posts/@securityaura/data-exfiltration-questions-and-how-to-answer-them-84856b14003c/","section":"Posts","summary":"","title":"Data Exfiltration: Questions and How to Answer Them","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]